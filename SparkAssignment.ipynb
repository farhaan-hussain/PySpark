{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhaan-hussain/PySpark/blob/main/SparkAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Scala Env."
      ],
      "metadata": {
        "id": "c6Z58K9N1_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78Oxr-PY1pA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7e66c9-3988-4199-849a-2672994507eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xyJiL9W-2lXu",
        "outputId": "f5174151-b528-45e8-d73c-669519e271fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Hello World in Scala"
      ],
      "metadata": {
        "id": "P86TPUCo6-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "qgcRIiBmsO-P",
        "outputId": "cb078933-4c2f-434f-b352-6aa6ca0876dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.4.1-bin-hadoop3  tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_aQP7Q28De",
        "outputId": "fe8514b4-b9a6-492d-ca01-9ed6d611ec55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/08/06 04:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Spark context Web UI available at http://1efc78f4a3be:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1754456202483).\n",
            "Spark session available as 'spark'.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "         \n",
            "Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 11.0.28)\n",
            "Type in expressions to have them evaluated.\n",
            "Type :help for more information.\n",
            "\u001b[35m\n",
            "scala> \u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "1gIQxLWBtq8J",
        "outputId": "a8c7a0fa-1cb4-47f1-d9bf-3ec96fd76864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.java  offices.csv\t   productlines.csv\t    tmp\n",
            "customers.csv\t orderdetails.csv  products.csv\n",
            "drive\t\t orders.csv\t   sample_data\n",
            "employees.csv\t payments.csv\t   spark-3.4.1-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oppMHVdzufkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Assignment.java\n",
        "import org.apache.spark.sql.*;\n",
        "import java.util.*;\n",
        "\n",
        "public class Assignment\n",
        "{\n",
        "  public static void main(String[] args)\n",
        "  {\n",
        "    SparkSession spark = SparkSession.builder().appName(\"Assignment\").getOrCreate();\n",
        "    DataSet<Row> customersDs = spark.read().option(\"header\", \"true\").csv(\"customers.csv\");\n",
        "    DataSet<Row> ordersDs = spark.read().option(\"header\", \"true\").csv(\"orders.csv\");\n",
        "    DataSet<Row> orderDetailsDs = spark.read().option(\"header\", \"true\").csv(\"orderdetails.csv\");\n",
        "    DataSet<Row> productsDs = spark.read().option(\"header\", \"true\").csv(\"products.csv\");\n",
        "    DataSet<Row> employeesDs = spark.read().option(\"header\", \"true\").csv(\"employees.csv\");\n",
        "    DataSet<Row> officesDs = spark.read().option(\"header\", \"true\").csv(\"offices.csv\");\n",
        "    DataSet<Row> productlineDs = spark.read().option(\"header\", \"true\").csv(\"productlines.csv\");\n",
        "    DataSet<Row> paymentsDs = spark.read().option(\"header\", \"true\").csv(\"payments.csv\");\n",
        "\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIaNIoC56kLN",
        "outputId": "e9e00fa8-d2ba-406e-92d0-32a1f5bf3d85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Assignment.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GuO1JQKwao5",
        "outputId": "c65920fc-98f2-4926-fc08-1c887ee91021"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.java  drive\t    sample_data\t\t     tmp\n",
            "csv_files\t paraquets  spark-3.4.1-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir paraquets"
      ],
      "metadata": {
        "id": "zCuZQUeAxz67"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Assignment.java\n",
        "import org.apache.spark.sql.*;\n",
        "import java.util.*;\n",
        "\n",
        "public class Assignment\n",
        "{\n",
        "  public static void main(String[] args)\n",
        "  {\n",
        "    SparkSession spark = SparkSession.builder().appName(\"Assignment\").master(\"local[*]\").getOrCreate();\n",
        "    Dataset<Row> customersDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/customers.csv\");\n",
        "    Dataset<Row> ordersDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/orders.csv\");\n",
        "    Dataset<Row> orderDetailsDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/orderdetails.csv\");\n",
        "    Dataset<Row> productsDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/products.csv\");\n",
        "    Dataset<Row> employeesDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/employees.csv\");\n",
        "    Dataset<Row> officesDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/offices.csv\");\n",
        "    Dataset<Row> productlineDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/productlines.csv\");\n",
        "    Dataset<Row> paymentsDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/payments.csv\");\n",
        "\n",
        "    customersDs.write().parquet(\"/content/paraquets/customers.parquet\");\n",
        "    ordersDs.write().parquet(\"/content/paraquets/orders.parquet\");\n",
        "    orderDetailsDs.write().parquet(\"/content/paraquets/orderdetails.parquet\");\n",
        "    productsDs.write().parquet(\"/content/paraquets/products.parquet\");\n",
        "    employeesDs.write().parquet(\"/content/paraquets/employees.parquet\");\n",
        "    officesDs.write().parquet(\"/content/paraquets/offices.parquet\");\n",
        "    productlineDs.write().parquet(\"/content/paraquets/productlines.parquet\");\n",
        "    paymentsDs.write().parquet(\"/content/paraquets/payments.parquet\");\n",
        "\n",
        "    spark.stop();\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu5d0K9ex7eY",
        "outputId": "a17d6254-7f15-4269-e831-1610d67ff04c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Assignment.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" Assignment.java"
      ],
      "metadata": {
        "id": "mb6kQ--NyI2U"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" \\\n",
        "  --add-exports java.base/sun.nio.ch=ALL-UNNAMED Assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjgemjUbzFBX",
        "outputId": "5f5dcdf6-41d1-4525-ac7f-514702172186"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:21:40 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:21:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:21:41 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:21:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:21:41 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:21:41 INFO SparkContext: Submitted application: Assignment\n",
            "25/08/06 05:21:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:21:41 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:21:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:21:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:21:41 INFO Utils: Successfully started service 'sparkDriver' on port 46467.\n",
            "25/08/06 05:21:41 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:21:41 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:21:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:21:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:21:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:21:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a4d088f6-af42-4d98-ab4f-def20350e30a\n",
            "25/08/06 05:21:42 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:21:42 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:21:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:21:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:21:42 INFO Executor: Starting executor ID driver on host 1efc78f4a3be\n",
            "25/08/06 05:21:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:21:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39867.\n",
            "25/08/06 05:21:42 INFO NettyBlockTransferService: Server created on 1efc78f4a3be:39867\n",
            "25/08/06 05:21:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:21:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:42 INFO BlockManagerMasterEndpoint: Registering block manager 1efc78f4a3be:39867 with 1767.6 MiB RAM, BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:21:43 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:21:45 INFO InMemoryFileIndex: It took 94 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:45 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:51 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 05:21:52 INFO CodeGenerator: Code generated in 469.097519 ms\n",
            "25/08/06 05:21:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:21:53 INFO SparkContext: Created broadcast 0 from csv at Assignment.java:9\n",
            "25/08/06 05:21:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:53 INFO SparkContext: Starting job: csv at Assignment.java:9\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Got job 0 (csv at Assignment.java:9) with 1 output partitions\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Final stage: ResultStage 0 (csv at Assignment.java:9)\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Assignment.java:9), which has no missing parents\n",
            "25/08/06 05:21:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:21:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Assignment.java:9) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:21:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 05:21:54 INFO FileScanRDD: Reading File path: file:///content/csv_files/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 05:21:54 INFO CodeGenerator: Code generated in 50.382793 ms\n",
            "25/08/06 05:21:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1743 bytes result sent to driver\n",
            "25/08/06 05:21:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 622 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:54 INFO DAGScheduler: ResultStage 0 (csv at Assignment.java:9) finished in 0.978 s\n",
            "25/08/06 05:21:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 05:21:54 INFO DAGScheduler: Job 0 finished: csv at Assignment.java:9, took 1.108503 s\n",
            "25/08/06 05:21:54 INFO CodeGenerator: Code generated in 32.281316 ms\n",
            "25/08/06 05:21:54 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:21:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:21:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:54 INFO SparkContext: Created broadcast 2 from csv at Assignment.java:9\n",
            "25/08/06 05:21:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:54 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:54 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#43, None)) > 0)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 3 from csv at Assignment.java:10\n",
            "25/08/06 05:21:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:55 INFO SparkContext: Starting job: csv at Assignment.java:10\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Got job 1 (csv at Assignment.java:10) with 1 output partitions\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Final stage: ResultStage 1 (csv at Assignment.java:10)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at csv at Assignment.java:10), which has no missing parents\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at csv at Assignment.java:10) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) \n",
            "25/08/06 05:21:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 05:21:55 INFO FileScanRDD: Reading File path: file:///content/csv_files/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 05:21:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1679 bytes result sent to driver\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: ResultStage 1 (csv at Assignment.java:10) finished in 0.090 s\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 1 finished: csv at Assignment.java:10, took 0.101584 s\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 198.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 5 from csv at Assignment.java:10\n",
            "25/08/06 05:21:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#74, None)) > 0)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 198.5 KiB, free 1766.5 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 6 from csv at Assignment.java:11\n",
            "25/08/06 05:21:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:55 INFO SparkContext: Starting job: csv at Assignment.java:11\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Got job 2 (csv at Assignment.java:11) with 1 output partitions\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Final stage: ResultStage 2 (csv at Assignment.java:11)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[23] at csv at Assignment.java:11), which has no missing parents\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.1 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[23] at csv at Assignment.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:21:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 05:21:55 INFO FileScanRDD: Reading File path: file:///content/csv_files/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 05:21:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1678 bytes result sent to driver\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 68 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: ResultStage 2 (csv at Assignment.java:11) finished in 0.111 s\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 2 finished: csv at Assignment.java:11, took 0.130177 s\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 198.5 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 8 from csv at Assignment.java:11\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#101, None)) > 0)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 198.5 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 9 from csv at Assignment.java:12\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO SparkContext: Starting job: csv at Assignment.java:12\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Got job 3 (csv at Assignment.java:12) with 1 output partitions\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Final stage: ResultStage 3 (csv at Assignment.java:12)\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[33] at csv at Assignment.java:12), which has no missing parents\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.1 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[33] at csv at Assignment.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:21:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 05:21:56 INFO FileScanRDD: Reading File path: file:///content/csv_files/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 05:21:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1698 bytes result sent to driver\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 65 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:56 INFO DAGScheduler: ResultStage 3 (csv at Assignment.java:12) finished in 0.096 s\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 3 finished: csv at Assignment.java:12, took 0.110217 s\n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 198.5 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 11 from csv at Assignment.java:12\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#136, None)) > 0)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.5 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 12 from csv at Assignment.java:13\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO SparkContext: Starting job: csv at Assignment.java:13\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Got job 4 (csv at Assignment.java:13) with 1 output partitions\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Final stage: ResultStage 4 (csv at Assignment.java:13)\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[43] at csv at Assignment.java:13), which has no missing parents\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.1 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[43] at csv at Assignment.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:21:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 05:21:56 INFO FileScanRDD: Reading File path: file:///content/csv_files/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 05:21:56 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1646 bytes result sent to driver\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 33 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:56 INFO DAGScheduler: ResultStage 4 (csv at Assignment.java:13) finished in 0.051 s\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 4 finished: csv at Assignment.java:13, took 0.064690 s\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.5 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 14 from csv at Assignment.java:13\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#169, None)) > 0)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 198.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 15 from csv at Assignment.java:14\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Starting job: csv at Assignment.java:14\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Got job 5 (csv at Assignment.java:14) with 1 output partitions\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Final stage: ResultStage 5 (csv at Assignment.java:14)\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[53] at csv at Assignment.java:14), which has no missing parents\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 12.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[53] at csv at Assignment.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 05:21:57 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 05:21:57 INFO FileScanRDD: Reading File path: file:///content/csv_files/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 05:21:57 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1648 bytes result sent to driver\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 29 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:57 INFO DAGScheduler: ResultStage 5 (csv at Assignment.java:14) finished in 0.051 s\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 5 finished: csv at Assignment.java:14, took 0.060428 s\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 198.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 17 from csv at Assignment.java:14\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#204, None)) > 0)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 198.5 KiB, free 1766.5 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 18 from csv at Assignment.java:15\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO SparkContext: Starting job: csv at Assignment.java:15\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Got job 6 (csv at Assignment.java:15) with 1 output partitions\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Final stage: ResultStage 6 (csv at Assignment.java:15)\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[63] at csv at Assignment.java:15), which has no missing parents\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 12.1 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[63] at csv at Assignment.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:21:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 05:21:57 INFO FileScanRDD: Reading File path: file:///content/csv_files/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 05:21:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1617 bytes result sent to driver\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 29 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:57 INFO DAGScheduler: ResultStage 6 (csv at Assignment.java:15) finished in 0.045 s\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 6 finished: csv at Assignment.java:15, took 0.050605 s\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 198.5 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 20 from csv at Assignment.java:15\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#229, None)) > 0)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 198.5 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 21 from csv at Assignment.java:16\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:58 INFO SparkContext: Starting job: csv at Assignment.java:16\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Got job 7 (csv at Assignment.java:16) with 1 output partitions\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Final stage: ResultStage 7 (csv at Assignment.java:16)\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[73] at csv at Assignment.java:16), which has no missing parents\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 12.1 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[73] at csv at Assignment.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:21:58 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 05:21:58 INFO FileScanRDD: Reading File path: file:///content/csv_files/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 05:21:58 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1614 bytes result sent to driver\n",
            "25/08/06 05:21:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 34 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:58 INFO DAGScheduler: ResultStage 7 (csv at Assignment.java:16) finished in 0.053 s\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Job 7 finished: csv at Assignment.java:16, took 0.061478 s\n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 198.5 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 23 from csv at Assignment.java:16\n",
            "25/08/06 05:21:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 198.5 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 24 from parquet at Assignment.java:18\n",
            "25/08/06 05:21:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:58 INFO SparkContext: Starting job: parquet at Assignment.java:18\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Got job 8 (parquet at Assignment.java:18) with 1 output partitions\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at Assignment.java:18)\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[82] at parquet at Assignment.java:18), which has no missing parents\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 213.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 1765.2 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 1efc78f4a3be:39867 (size: 76.9 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[82] at parquet at Assignment.java:18) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:58 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:21:58 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:21:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:21:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:21:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary salesRepEmployeeNumber (STRING);\n",
            "  optional binary creditLimit (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:21:58 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:59 INFO FileScanRDD: Reading File path: file:///content/csv_files/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 05:21:59 INFO CodeGenerator: Code generated in 43.106402 ms\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: Saved output of task 'attempt_20250806052158197197136055836895_0008_m_000000_8' to file:/content/paraquets/customers.parquet/_temporary/0/task_20250806052158197197136055836895_0008_m_000000\n",
            "25/08/06 05:22:00 INFO SparkHadoopMapRedUtil: attempt_20250806052158197197136055836895_0008_m_000000_8: Committed. Elapsed time: 8 ms.\n",
            "25/08/06 05:22:00 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2545 bytes result sent to driver\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 1588 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:00 INFO DAGScheduler: ResultStage 8 (parquet at Assignment.java:18) finished in 1.670 s\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 8 finished: parquet at Assignment.java:18, took 1.675159 s\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Start to commit write Job 076067cf-9666-4c84-bfcd-93850b53be27.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Write Job 076067cf-9666-4c84-bfcd-93850b53be27 committed. Elapsed time: 41 ms.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Finished processing stats for write job 076067cf-9666-4c84-bfcd-93850b53be27.\n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 26 from parquet at Assignment.java:19\n",
            "25/08/06 05:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:00 INFO SparkContext: Starting job: parquet at Assignment.java:19\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Got job 9 (parquet at Assignment.java:19) with 1 output partitions\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at Assignment.java:19)\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[85] at parquet at Assignment.java:19), which has no missing parents\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 211.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 1efc78f4a3be:39867 (size: 76.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[85] at parquet at Assignment.java:19) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) \n",
            "25/08/06 05:22:00 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"requiredDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"shippedDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"status\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"comments\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary orderNumber (STRING);\n",
            "  optional binary orderDate (STRING);\n",
            "  optional binary requiredDate (STRING);\n",
            "  optional binary shippedDate (STRING);\n",
            "  optional binary status (STRING);\n",
            "  optional binary comments (STRING);\n",
            "  optional binary customerNumber (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:00 INFO FileScanRDD: Reading File path: file:///content/csv_files/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 05:22:00 INFO CodeGenerator: Code generated in 51.434523 ms\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522006319789049353731045_0009_m_000000_9' to file:/content/paraquets/orders.parquet/_temporary/0/task_202508060522006319789049353731045_0009_m_000000\n",
            "25/08/06 05:22:00 INFO SparkHadoopMapRedUtil: attempt_202508060522006319789049353731045_0009_m_000000_9: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 05:22:00 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 242 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:00 INFO DAGScheduler: ResultStage 9 (parquet at Assignment.java:19) finished in 0.290 s\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 9 finished: parquet at Assignment.java:19, took 0.298467 s\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Start to commit write Job e5a6e2f9-8574-4515-8112-e5bab1f766ad.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Write Job e5a6e2f9-8574-4515-8112-e5bab1f766ad committed. Elapsed time: 21 ms.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Finished processing stats for write job e5a6e2f9-8574-4515-8112-e5bab1f766ad.\n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 28 from parquet at Assignment.java:20\n",
            "25/08/06 05:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:00 INFO SparkContext: Starting job: parquet at Assignment.java:20\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Got job 10 (parquet at Assignment.java:20) with 1 output partitions\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at Assignment.java:20)\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[88] at parquet at Assignment.java:20), which has no missing parents\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 211.1 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 1efc78f4a3be:39867 (size: 76.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[88] at parquet at Assignment.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:22:00 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary orderNumber (STRING);\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary quantityOrdered (STRING);\n",
            "  optional binary priceEach (STRING);\n",
            "  optional binary orderLineNumber (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:00 INFO FileScanRDD: Reading File path: file:///content/csv_files/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 05:22:00 INFO CodeGenerator: Code generated in 35.603158 ms\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522001461962991534138916_0010_m_000000_10' to file:/content/paraquets/orderdetails.parquet/_temporary/0/task_202508060522001461962991534138916_0010_m_000000\n",
            "25/08/06 05:22:01 INFO SparkHadoopMapRedUtil: attempt_202508060522001461962991534138916_0010_m_000000_10: Committed. Elapsed time: 5 ms.\n",
            "25/08/06 05:22:01 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:01 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 240 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:01 INFO DAGScheduler: ResultStage 10 (parquet at Assignment.java:20) finished in 0.283 s\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 10 finished: parquet at Assignment.java:20, took 0.288455 s\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Start to commit write Job 97551e3c-c873-45d2-9871-8821d8a07270.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Write Job 97551e3c-c873-45d2-9871-8821d8a07270 committed. Elapsed time: 27 ms.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Finished processing stats for write job 97551e3c-c873-45d2-9871-8821d8a07270.\n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 198.5 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.8 MiB)\n",
            "25/08/06 05:22:01 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:01 INFO SparkContext: Created broadcast 30 from parquet at Assignment.java:21\n",
            "25/08/06 05:22:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:01 INFO SparkContext: Starting job: parquet at Assignment.java:21\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Got job 11 (parquet at Assignment.java:21) with 1 output partitions\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at Assignment.java:21)\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[91] at parquet at Assignment.java:21), which has no missing parents\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 212.0 KiB, free 1765.6 MiB)\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1765.6 MiB)\n",
            "25/08/06 05:22:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 1efc78f4a3be:39867 (size: 76.6 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:01 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[91] at parquet at Assignment.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:01 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:22:01 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:01 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:01 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productScale\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productVendor\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityInStock\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"buyPrice\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"MSRP\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary productScale (STRING);\n",
            "  optional binary productVendor (STRING);\n",
            "  optional binary productDescription (STRING);\n",
            "  optional binary quantityInStock (STRING);\n",
            "  optional binary buyPrice (STRING);\n",
            "  optional binary MSRP (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:01 INFO FileScanRDD: Reading File path: file:///content/csv_files/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 05:22:01 INFO CodeGenerator: Code generated in 35.486986 ms\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522017822383054524772237_0011_m_000000_11' to file:/content/paraquets/products.parquet/_temporary/0/task_202508060522017822383054524772237_0011_m_000000\n",
            "25/08/06 05:22:01 INFO SparkHadoopMapRedUtil: attempt_202508060522017822383054524772237_0011_m_000000_11: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:01 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:01 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 509 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:01 INFO DAGScheduler: ResultStage 11 (parquet at Assignment.java:21) finished in 0.579 s\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 11 finished: parquet at Assignment.java:21, took 0.588556 s\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Start to commit write Job 0cb62299-9a18-41c3-b069-f5428a0bd300.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Write Job 0cb62299-9a18-41c3-b069-f5428a0bd300 committed. Elapsed time: 37 ms.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Finished processing stats for write job 0cb62299-9a18-41c3-b069-f5428a0bd300.\n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 198.5 KiB, free 1765.4 MiB)\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 32 from parquet at Assignment.java:22\n",
            "25/08/06 05:22:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:02 INFO SparkContext: Starting job: parquet at Assignment.java:22\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Got job 12 (parquet at Assignment.java:22) with 1 output partitions\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Assignment.java:22)\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[94] at parquet at Assignment.java:22), which has no missing parents\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 211.8 KiB, free 1765.1 MiB)\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1765.1 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 1efc78f4a3be:39867 (size: 76.5 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[94] at parquet at Assignment.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:22:02 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"firstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"extension\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"email\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"reportsTo\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"jobTitle\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary employeeNumber (STRING);\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary firstName (STRING);\n",
            "  optional binary extension (STRING);\n",
            "  optional binary email (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary reportsTo (STRING);\n",
            "  optional binary jobTitle (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:02 INFO FileScanRDD: Reading File path: file:///content/csv_files/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 05:22:02 INFO CodeGenerator: Code generated in 70.165751 ms\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522028991684213158794495_0012_m_000000_12' to file:/content/paraquets/employees.parquet/_temporary/0/task_202508060522028991684213158794495_0012_m_000000\n",
            "25/08/06 05:22:02 INFO SparkHadoopMapRedUtil: attempt_202508060522028991684213158794495_0012_m_000000_12: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:02 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2502 bytes result sent to driver\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 220 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:02 INFO DAGScheduler: ResultStage 12 (parquet at Assignment.java:22) finished in 0.279 s\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 12 finished: parquet at Assignment.java:22, took 0.286910 s\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Start to commit write Job 737d908b-2acd-456d-86a6-15850f57c2aa.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Write Job 737d908b-2acd-456d-86a6-15850f57c2aa committed. Elapsed time: 19 ms.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Finished processing stats for write job 737d908b-2acd-456d-86a6-15850f57c2aa.\n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 198.5 KiB, free 1764.9 MiB)\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.8 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 34 from parquet at Assignment.java:23\n",
            "25/08/06 05:22:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:02 INFO SparkContext: Starting job: parquet at Assignment.java:23\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Got job 13 (parquet at Assignment.java:23) with 1 output partitions\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Assignment.java:23)\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[97] at parquet at Assignment.java:23), which has no missing parents\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 212.0 KiB, free 1764.6 MiB)\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1764.5 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 1efc78f4a3be:39867 (size: 76.4 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[97] at parquet at Assignment.java:23) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 05:22:02 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"territory\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary territory (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:02 INFO FileScanRDD: Reading File path: file:///content/csv_files/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522029066046437883789402_0013_m_000000_13' to file:/content/paraquets/offices.parquet/_temporary/0/task_202508060522029066046437883789402_0013_m_000000\n",
            "25/08/06 05:22:02 INFO SparkHadoopMapRedUtil: attempt_202508060522029066046437883789402_0013_m_000000_13: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:02 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 215 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:02 INFO DAGScheduler: ResultStage 13 (parquet at Assignment.java:23) finished in 0.284 s\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 13 finished: parquet at Assignment.java:23, took 0.290576 s\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Start to commit write Job ef241786-2017-4db7-a5c9-780ae32cb6c3.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Write Job ef241786-2017-4db7-a5c9-780ae32cb6c3 committed. Elapsed time: 29 ms.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Finished processing stats for write job ef241786-2017-4db7-a5c9-780ae32cb6c3.\n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 198.5 KiB, free 1764.4 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.3 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1766.9 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 36 from parquet at Assignment.java:24\n",
            "25/08/06 05:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:03 INFO SparkContext: Starting job: parquet at Assignment.java:24\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Got job 14 (parquet at Assignment.java:24) with 1 output partitions\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Assignment.java:24)\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[100] at parquet at Assignment.java:24), which has no missing parents\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 210.9 KiB, free 1764.1 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1764.0 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 1efc78f4a3be:39867 (size: 76.3 KiB, free: 1766.8 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[100] at parquet at Assignment.java:24) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:03 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:22:03 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 1efc78f4a3be:39867 in memory (size: 76.6 KiB, free: 1766.9 MiB)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"htmlDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"image\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "  optional binary htmlDescription (STRING);\n",
            "  optional binary image (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 1efc78f4a3be:39867 in memory (size: 76.4 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 1efc78f4a3be:39867 in memory (size: 76.5 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:03 INFO FileScanRDD: Reading File path: file:///content/csv_files/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:03 INFO CodeGenerator: Code generated in 31.203658 ms\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522033384327301066927734_0014_m_000000_14' to file:/content/paraquets/productlines.parquet/_temporary/0/task_202508060522033384327301066927734_0014_m_000000\n",
            "25/08/06 05:22:03 INFO SparkHadoopMapRedUtil: attempt_202508060522033384327301066927734_0014_m_000000_14: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:03 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2502 bytes result sent to driver\n",
            "25/08/06 05:22:03 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 422 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:03 INFO DAGScheduler: ResultStage 14 (parquet at Assignment.java:24) finished in 0.524 s\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Job 14 finished: parquet at Assignment.java:24, took 0.529915 s\n",
            "25/08/06 05:22:03 INFO FileFormatWriter: Start to commit write Job 5feaae44-463c-4fd4-a31e-89897092a4fc.\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:03 INFO FileFormatWriter: Write Job 5feaae44-463c-4fd4-a31e-89897092a4fc committed. Elapsed time: 25 ms.\n",
            "25/08/06 05:22:03 INFO FileFormatWriter: Finished processing stats for write job 5feaae44-463c-4fd4-a31e-89897092a4fc.\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:03 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 1efc78f4a3be:39867 in memory (size: 76.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 1efc78f4a3be:39867 in memory (size: 76.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 1efc78f4a3be:39867 in memory (size: 76.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 38 from parquet at Assignment.java:25\n",
            "25/08/06 05:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:03 INFO SparkContext: Starting job: parquet at Assignment.java:25\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Got job 15 (parquet at Assignment.java:25) with 1 output partitions\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at Assignment.java:25)\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[103] at parquet at Assignment.java:25), which has no missing parents\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 210.9 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 1efc78f4a3be:39867 (size: 76.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[103] at parquet at Assignment.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:03 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:22:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"amount\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional binary paymentDate (STRING);\n",
            "  optional binary amount (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:03 INFO FileScanRDD: Reading File path: file:///content/csv_files/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 05:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522032169946207929521064_0015_m_000000_15' to file:/content/paraquets/payments.parquet/_temporary/0/task_202508060522032169946207929521064_0015_m_000000\n",
            "25/08/06 05:22:04 INFO SparkHadoopMapRedUtil: attempt_202508060522032169946207929521064_0015_m_000000_15: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 186 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:04 INFO DAGScheduler: ResultStage 15 (parquet at Assignment.java:25) finished in 0.271 s\n",
            "25/08/06 05:22:04 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 05:22:04 INFO DAGScheduler: Job 15 finished: parquet at Assignment.java:25, took 0.274485 s\n",
            "25/08/06 05:22:04 INFO FileFormatWriter: Start to commit write Job 5d634d85-610f-488f-913a-30e31077d583.\n",
            "25/08/06 05:22:04 INFO FileFormatWriter: Write Job 5d634d85-610f-488f-913a-30e31077d583 committed. Elapsed time: 34 ms.\n",
            "25/08/06 05:22:04 INFO FileFormatWriter: Finished processing stats for write job 5d634d85-610f-488f-913a-30e31077d583.\n",
            "25/08/06 05:22:04 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:22:04 INFO SparkUI: Stopped Spark web UI at http://1efc78f4a3be:4040\n",
            "25/08/06 05:22:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:22:04 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:22:04 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:22:04 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:22:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:22:04 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:22:04 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:22:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-1235662a-4791-42bb-ac45-58f82b5ad1cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW2hGhe1zmLS",
        "outputId": "1404007b-3ab9-4490-bfd1-ecfd6f017657"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.class  csv_files  paraquets\t  spark-3.4.1-bin-hadoop3\n",
            "Assignment.java   drive      sample_data  tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glKC8MUk0Bwx",
        "outputId": "6f8e11a8-da18-422c-8cdd-2c98ddaf475b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-u1EmEO0KFj",
        "outputId": "9ebbd3c4-9247-415d-b007-9a78d96b5e21"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.class  \u001b[0m\u001b[01;34mcsv_files\u001b[0m/  \u001b[01;34mparaquets\u001b[0m/    \u001b[01;34mspark-3.4.1-bin-hadoop3\u001b[0m/\n",
            "Assignment.java   \u001b[01;34mdrive\u001b[0m/      \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtmp\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "import java.util.*;\n",
        "\n",
        "public class task2\n",
        "{\n",
        "  public static void main(String[] args)\n",
        "  {\n",
        "    SparkSession spark = SparkSession.builder().appName(\"Assignment\").master(\"local[*]\").getOrCreate();\n",
        "    Dataset<Row> orderDs = spark.read().parquet(\"/content/paraquets/orders.parquet\");\n",
        "    Dataset<Row>productsDs = spark.read().parquet(\"/content/paraquets/products.parquet\");\n",
        "    Dataset<Row>orderDetailsDs = spark.read().parquet(\"/content/paraquets/orderdetails.parquet\");\n",
        "    Dataset<Row>topProductsDs = orderDetailsDs.groupBy(\"productCode\").agg(sum(\"quantityOrdered\").alias(\"quantityOrdered\")).join(productsDs, \"productCode\").orderBy(desc(\"quantityOrdered\")).limit(10);\n",
        "    Dataset<Row> productRevenueDs = orderDetailsDs\n",
        "                .withColumn(\"revenue\", col(\"quantityOrdered\").multiply(col(\"priceEach\")))\n",
        "                .join(productsDs, \"productCode\")\n",
        "                .groupBy(\"productCode\", \"productName\")\n",
        "                .agg(sum(\"revenue\").alias(\"productRevenue\"))\n",
        "                .orderBy(desc(\"productRevenue\"));\n",
        "\n",
        "    Dataset<Row>avgOrderValueDs = orderDs.join(orderDetailsDs,\"orderNumber\").groupBy(col(\"customerNumber\")).agg(avg(col(\"quantityOrdered\").multiply(col(\"priceEach\"))).alias(\"avgOrderValue\"));\n",
        "\n",
        "\n",
        "    productRevenueDs.write().parquet(\"/content/paraquets/productRevenueDs.parquet\");\n",
        "    avgOrderValueDs.write().parquet(\"/content/paraquets/avgOrderValueDs.parquet\");\n",
        "    spark.stop();\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5I99iDy1J8H",
        "outputId": "5086f24d-a7f2-4abc-f9a2-ee5169265c26"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" task2.java"
      ],
      "metadata": {
        "id": "HBn-Icpa8lBs"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" \\\n",
        "  --add-exports java.base/sun.nio.ch=ALL-UNNAMED task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oYYQ405_9646",
        "outputId": "b32d9c22-8103-4015-dbb9-61d0cff9c402"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 06:31:37 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 06:31:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 06:31:37 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:31:37 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 06:31:37 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:31:37 INFO SparkContext: Submitted application: Assignment\n",
            "25/08/06 06:31:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 06:31:37 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 06:31:37 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 06:31:37 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 06:31:37 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 06:31:37 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 06:31:37 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 06:31:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 06:31:38 INFO Utils: Successfully started service 'sparkDriver' on port 40455.\n",
            "25/08/06 06:31:38 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 06:31:38 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 06:31:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 06:31:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 06:31:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 06:31:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f977543a-92ae-42a4-8007-666bb768ecd8\n",
            "25/08/06 06:31:38 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 06:31:38 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 06:31:38 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 06:31:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 06:31:39 INFO Executor: Starting executor ID driver on host 1efc78f4a3be\n",
            "25/08/06 06:31:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 06:31:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45575.\n",
            "25/08/06 06:31:39 INFO NettyBlockTransferService: Server created on 1efc78f4a3be:45575\n",
            "25/08/06 06:31:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 06:31:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1efc78f4a3be, 45575, None)\n",
            "25/08/06 06:31:39 INFO BlockManagerMasterEndpoint: Registering block manager 1efc78f4a3be:45575 with 1767.6 MiB RAM, BlockManagerId(driver, 1efc78f4a3be, 45575, None)\n",
            "25/08/06 06:31:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1efc78f4a3be, 45575, None)\n",
            "25/08/06 06:31:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1efc78f4a3be, 45575, None)\n",
            "25/08/06 06:31:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 06:31:40 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 06:31:41 INFO InMemoryFileIndex: It took 112 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:31:42 INFO SparkContext: Starting job: parquet at task2.java:10\n",
            "25/08/06 06:31:42 INFO DAGScheduler: Got job 0 (parquet at task2.java:10) with 1 output partitions\n",
            "25/08/06 06:31:42 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at task2.java:10)\n",
            "25/08/06 06:31:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:31:42 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at task2.java:10), which has no missing parents\n",
            "25/08/06 06:31:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:31:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:31:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1efc78f4a3be:45575 (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at task2.java:10) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes) \n",
            "25/08/06 06:31:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 06:31:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2070 bytes result sent to driver\n",
            "25/08/06 06:31:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 829 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:43 INFO DAGScheduler: ResultStage 0 (parquet at task2.java:10) finished in 1.138 s\n",
            "25/08/06 06:31:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:31:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 06:31:43 INFO DAGScheduler: Job 0 finished: parquet at task2.java:10, took 1.260632 s\n",
            "25/08/06 06:31:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1efc78f4a3be:45575 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:46 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:31:46 INFO SparkContext: Starting job: parquet at task2.java:11\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Got job 1 (parquet at task2.java:11) with 1 output partitions\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at task2.java:11)\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at task2.java:11), which has no missing parents\n",
            "25/08/06 06:31:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:31:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:31:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1efc78f4a3be:45575 (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at task2.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7566 bytes) \n",
            "25/08/06 06:31:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 06:31:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2124 bytes result sent to driver\n",
            "25/08/06 06:31:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 108 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:46 INFO DAGScheduler: ResultStage 1 (parquet at task2.java:11) finished in 0.198 s\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:31:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 06:31:46 INFO DAGScheduler: Job 1 finished: parquet at task2.java:11, took 0.211332 s\n",
            "25/08/06 06:31:46 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:31:47 INFO SparkContext: Starting job: parquet at task2.java:12\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Got job 2 (parquet at task2.java:12) with 1 output partitions\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at task2.java:12)\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at task2.java:12), which has no missing parents\n",
            "25/08/06 06:31:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:31:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.3 MiB)\n",
            "25/08/06 06:31:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1efc78f4a3be:45575 (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:47 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at task2.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 06:31:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 06:31:47 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1955 bytes result sent to driver\n",
            "25/08/06 06:31:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:47 INFO DAGScheduler: ResultStage 2 (parquet at task2.java:12) finished in 0.152 s\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:31:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 06:31:47 INFO DAGScheduler: Job 2 finished: parquet at task2.java:12, took 0.159951 s\n",
            "25/08/06 06:31:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1efc78f4a3be:45575 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1efc78f4a3be:45575 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:31:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#33)\n",
            "25/08/06 06:31:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:31:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#14)\n",
            "25/08/06 06:31:52 INFO CodeGenerator: Code generated in 876.391318 ms\n",
            "25/08/06 06:31:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:31:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:31:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1efc78f4a3be:45575 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:52 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:31:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:31:52 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:31:52 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:31:52 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:31:52 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:31:52 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:52 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:31:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:31:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:31:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1efc78f4a3be:45575 (size: 6.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:52 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:52 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:52 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7990 bytes) \n",
            "25/08/06 06:31:52 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 06:31:52 INFO FileScanRDD: Reading File path: file:///content/paraquets/products.parquet/part-00000-6eb27d84-cfb8-4bb7-8aff-54ca7a5271d7-c000.snappy.parquet, range: 0-16667, partition values: [empty row]\n",
            "25/08/06 06:31:53 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 06:31:53 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 06:31:53 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 5505 bytes result sent to driver\n",
            "25/08/06 06:31:53 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1189 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:53 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:53 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.232 s\n",
            "25/08/06 06:31:53 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:31:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 06:31:53 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.259489 s\n",
            "25/08/06 06:31:53 INFO CodeGenerator: Code generated in 23.89766 ms\n",
            "25/08/06 06:31:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.0 MiB, free 1735.3 MiB)\n",
            "25/08/06 06:31:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1735.3 MiB)\n",
            "25/08/06 06:31:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1efc78f4a3be:45575 (size: 4.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:31:54 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:31:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:31:54 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#33)\n",
            "25/08/06 06:31:54 INFO CodeGenerator: Code generated in 246.364714 ms\n",
            "25/08/06 06:31:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.3 KiB, free 1735.1 MiB)\n",
            "25/08/06 06:31:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1735.1 MiB)\n",
            "25/08/06 06:31:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1efc78f4a3be:45575 (size: 34.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:54 INFO SparkContext: Created broadcast 6 from parquet at task2.java:24\n",
            "25/08/06 06:31:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Registering RDD 13 (parquet at task2.java:24) as input to shuffle 0\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Got map stage job 4 (parquet at task2.java:24) with 1 output partitions\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (parquet at task2.java:24)\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at parquet at task2.java:24), which has no missing parents\n",
            "25/08/06 06:31:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 52.1 KiB, free 1735.1 MiB)\n",
            "25/08/06 06:31:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 1735.0 MiB)\n",
            "25/08/06 06:31:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1efc78f4a3be:45575 (size: 22.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at parquet at task2.java:24) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 06:31:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 06:31:54 INFO CodeGenerator: Code generated in 23.111067 ms\n",
            "25/08/06 06:31:54 INFO CodeGenerator: Code generated in 15.002125 ms\n",
            "25/08/06 06:31:54 INFO CodeGenerator: Code generated in 28.340112 ms\n",
            "25/08/06 06:31:55 INFO CodeGenerator: Code generated in 28.073877 ms\n",
            "25/08/06 06:31:55 INFO FileScanRDD: Reading File path: file:///content/paraquets/orderdetails.parquet/part-00000-c26ec8af-f0f3-4b35-bc77-b65e9e3f1da1-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 06:31:55 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 06:31:55 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3926 bytes result sent to driver\n",
            "25/08/06 06:31:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 517 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:55 INFO DAGScheduler: ShuffleMapStage 4 (parquet at task2.java:24) finished in 0.564 s\n",
            "25/08/06 06:31:55 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:31:55 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:31:55 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:31:55 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:31:55 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:31:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:31:55 INFO CodeGenerator: Code generated in 30.255671 ms\n",
            "25/08/06 06:31:55 INFO CodeGenerator: Code generated in 34.932999 ms\n",
            "25/08/06 06:31:55 INFO SparkContext: Starting job: parquet at task2.java:24\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Got job 5 (parquet at task2.java:24) with 1 output partitions\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at task2.java:24)\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[18] at parquet at task2.java:24), which has no missing parents\n",
            "25/08/06 06:31:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 49.6 KiB, free 1735.0 MiB)\n",
            "25/08/06 06:31:55 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 22.3 KiB, free 1735.0 MiB)\n",
            "25/08/06 06:31:55 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1efc78f4a3be:45575 (size: 22.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:55 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[18] at parquet at task2.java:24) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:55 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:55 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (1efc78f4a3be, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 06:31:55 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
            "25/08/06 06:31:55 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:31:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\n",
            "25/08/06 06:31:55 INFO CodeGenerator: Code generated in 26.889882 ms\n",
            "25/08/06 06:31:55 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 10041 bytes result sent to driver\n",
            "25/08/06 06:31:55 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 171 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:55 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:55 INFO DAGScheduler: ResultStage 6 (parquet at task2.java:24) finished in 0.209 s\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:31:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Job 5 finished: parquet at task2.java:24, took 0.253916 s\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Registering RDD 19 (parquet at task2.java:24) as input to shuffle 1\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Got map stage job 6 (parquet at task2.java:24) with 1 output partitions\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (parquet at task2.java:24)\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[19] at parquet at task2.java:24), which has no missing parents\n",
            "25/08/06 06:31:55 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 53.2 KiB, free 1734.9 MiB)\n",
            "25/08/06 06:31:55 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1734.9 MiB)\n",
            "25/08/06 06:31:55 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1efc78f4a3be:45575 (size: 23.5 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:55 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[19] at parquet at task2.java:24) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:31:55 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:31:55 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (1efc78f4a3be, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 06:31:55 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)\n",
            "25/08/06 06:31:55 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:31:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 06:31:56 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1efc78f4a3be:45575 in memory (size: 22.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:56 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1efc78f4a3be:45575 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 6571 bytes result sent to driver\n",
            "25/08/06 06:31:56 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 1efc78f4a3be:45575 in memory (size: 22.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:31:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 230 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 06:31:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:31:56 INFO DAGScheduler: ShuffleMapStage 8 (parquet at task2.java:24) finished in 0.273 s\n",
            "25/08/06 06:31:56 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:31:56 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:31:56 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:31:56 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:31:56 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_ALREADY_EXISTS] Path file:/content/paraquets/productRevenueDs.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.outputPathAlreadyExistsError(QueryCompilationErrors.scala:1585)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:123)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n",
            "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n",
            "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
            "\tat task2.main(task2.java:24)\n",
            "25/08/06 06:31:56 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 06:31:56 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 06:31:56 INFO SparkUI: Stopped Spark web UI at http://1efc78f4a3be:4040\n",
            "25/08/06 06:31:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 06:31:56 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 06:31:56 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 06:31:56 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 06:31:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 06:31:56 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 06:31:56 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 06:31:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff584ed-79d3-4724-9993-e26d3da41576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "import java.util.*;\n",
        "\n",
        "public class task3\n",
        "{\n",
        "  public static void main(String[] args)\n",
        "  {\n",
        "    SparkSession spark = SparkSession.builder().appName(\"Assignment\").master(\"local[*]\").getOrCreate();\n",
        "\n",
        "    Dataset<Row> ordersDs = spark.read().parquet(\"/content/paraquets/orders.parquet\");\n",
        "    Dataset<Row> orderDetailsDs = spark.read().parquet(\"/content/paraquets/orderdetails.parquet\");\n",
        "    Dataset<Row> customersDs = spark.read().parquet(\"/content/paraquets/customers.parquet\");\n",
        "    Dataset<Row> employeesDs = spark.read().parquet(\"/content/paraquets/employees.parquet\");\n",
        "    Dataset<Row> officesDs = spark.read().parquet(\"/content/paraquets/offices.parquet\");\n",
        "    Dataset<Row> paymentsDs = spark.read().parquet(\"/content/paraquets/payments.parquet\");\n",
        "\n",
        "\n",
        "    Dataset<Row> totalRevenueCountryDs = customersDs.join(paymentsDs,\"customerNumber\").groupBy(\"country\").agg(sum(\"amount\"));\n",
        "    totalRevenueCountryDs.show();\n",
        "    totalRevenueCountryDs.write().mode(\"overwrite\").parquet(\"/content/paraquets/totalRevenueCountryDs.parquet\");\n",
        "    spark.stop();\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMcExJA3-yej",
        "outputId": "15d66dd8-1ca0-4a2a-ca5c-97544e45cd2d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" task3.java"
      ],
      "metadata": {
        "id": "BinNPskLIZPJ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" \\\n",
        "  --add-exports java.base/sun.nio.ch=ALL-UNNAMED task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n_g9QmbIeq4",
        "outputId": "2358d65e-c043-4a1d-e91f-9d07a9a9f6ca"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 07:04:22 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 07:04:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 07:04:22 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:04:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 07:04:22 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 07:04:22 INFO SparkContext: Submitted application: Assignment\n",
            "25/08/06 07:04:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 07:04:22 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 07:04:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 07:04:22 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 07:04:22 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 07:04:22 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 07:04:22 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 07:04:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 07:04:23 INFO Utils: Successfully started service 'sparkDriver' on port 37533.\n",
            "25/08/06 07:04:23 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 07:04:23 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 07:04:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 07:04:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 07:04:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 07:04:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5fb9bf70-0d8b-4e4f-9bd9-f1cf3eaf22bd\n",
            "25/08/06 07:04:23 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 07:04:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 07:04:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 07:04:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 07:04:24 INFO Executor: Starting executor ID driver on host 1efc78f4a3be\n",
            "25/08/06 07:04:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 07:04:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35437.\n",
            "25/08/06 07:04:24 INFO NettyBlockTransferService: Server created on 1efc78f4a3be:35437\n",
            "25/08/06 07:04:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 07:04:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1efc78f4a3be, 35437, None)\n",
            "25/08/06 07:04:24 INFO BlockManagerMasterEndpoint: Registering block manager 1efc78f4a3be:35437 with 1767.6 MiB RAM, BlockManagerId(driver, 1efc78f4a3be, 35437, None)\n",
            "25/08/06 07:04:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1efc78f4a3be, 35437, None)\n",
            "25/08/06 07:04:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1efc78f4a3be, 35437, None)\n",
            "25/08/06 07:04:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 07:04:25 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 07:04:27 INFO InMemoryFileIndex: It took 110 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:04:29 INFO SparkContext: Starting job: parquet at task3.java:11\n",
            "25/08/06 07:04:29 INFO DAGScheduler: Got job 0 (parquet at task3.java:11) with 1 output partitions\n",
            "25/08/06 07:04:29 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at task3.java:11)\n",
            "25/08/06 07:04:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at task3.java:11), which has no missing parents\n",
            "25/08/06 07:04:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:04:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:04:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1efc78f4a3be:35437 (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at task3.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes) \n",
            "25/08/06 07:04:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 07:04:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2070 bytes result sent to driver\n",
            "25/08/06 07:04:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 856 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:30 INFO DAGScheduler: ResultStage 0 (parquet at task3.java:11) finished in 1.208 s\n",
            "25/08/06 07:04:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 07:04:30 INFO DAGScheduler: Job 0 finished: parquet at task3.java:11, took 1.330318 s\n",
            "25/08/06 07:04:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1efc78f4a3be:35437 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:33 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:04:33 INFO SparkContext: Starting job: parquet at task3.java:12\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Got job 1 (parquet at task3.java:12) with 1 output partitions\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at task3.java:12)\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at task3.java:12), which has no missing parents\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.5 MiB)\n",
            "25/08/06 07:04:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1efc78f4a3be:35437 (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at task3.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes) \n",
            "25/08/06 07:04:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 07:04:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1912 bytes result sent to driver\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 62 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:33 INFO DAGScheduler: ResultStage 1 (parquet at task3.java:12) finished in 0.109 s\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 1 finished: parquet at task3.java:12, took 0.122359 s\n",
            "25/08/06 07:04:33 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:04:33 INFO SparkContext: Starting job: parquet at task3.java:13\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Got job 2 (parquet at task3.java:13) with 1 output partitions\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at task3.java:13)\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at task3.java:13), which has no missing parents\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.3 MiB)\n",
            "25/08/06 07:04:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1efc78f4a3be:35437 (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at task3.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7567 bytes) \n",
            "25/08/06 07:04:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 07:04:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2247 bytes result sent to driver\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 49 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:33 INFO DAGScheduler: ResultStage 2 (parquet at task3.java:13) finished in 0.119 s\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 2 finished: parquet at task3.java:13, took 0.126759 s\n",
            "25/08/06 07:04:33 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:04:33 INFO SparkContext: Starting job: parquet at task3.java:14\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Got job 3 (parquet at task3.java:14) with 1 output partitions\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at task3.java:14)\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at task3.java:14), which has no missing parents\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.2 MiB)\n",
            "25/08/06 07:04:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1efc78f4a3be:35437 (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at task3.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7567 bytes) \n",
            "25/08/06 07:04:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 07:04:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2059 bytes result sent to driver\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 44 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:33 INFO DAGScheduler: ResultStage 3 (parquet at task3.java:14) finished in 0.075 s\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 3 finished: parquet at task3.java:14, took 0.090268 s\n",
            "25/08/06 07:04:33 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:04:33 INFO SparkContext: Starting job: parquet at task3.java:15\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Got job 4 (parquet at task3.java:15) with 1 output partitions\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at task3.java:15)\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at parquet at task3.java:15), which has no missing parents\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 102.7 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:04:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 07:04:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1efc78f4a3be:35437 (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:33 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at parquet at task3.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7565 bytes) \n",
            "25/08/06 07:04:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 07:04:33 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2092 bytes result sent to driver\n",
            "25/08/06 07:04:33 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 49 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:33 INFO DAGScheduler: ResultStage 4 (parquet at task3.java:15) finished in 0.094 s\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 07:04:33 INFO DAGScheduler: Job 4 finished: parquet at task3.java:15, took 0.106436 s\n",
            "25/08/06 07:04:34 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 07:04:34 INFO SparkContext: Starting job: parquet at task3.java:16\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Got job 5 (parquet at task3.java:16) with 1 output partitions\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at task3.java:16)\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at parquet at task3.java:16), which has no missing parents\n",
            "25/08/06 07:04:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 102.7 KiB, free 1767.0 MiB)\n",
            "25/08/06 07:04:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 07:04:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1efc78f4a3be:35437 (size: 37.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:04:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at parquet at task3.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7566 bytes) \n",
            "25/08/06 07:04:34 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 07:04:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1905 bytes result sent to driver\n",
            "25/08/06 07:04:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 35 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:34 INFO DAGScheduler: ResultStage 5 (parquet at task3.java:16) finished in 0.062 s\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 07:04:34 INFO DAGScheduler: Job 5 finished: parquet at task3.java:16, took 0.069453 s\n",
            "25/08/06 07:04:35 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 1efc78f4a3be:35437 in memory (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1efc78f4a3be:35437 in memory (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:35 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1efc78f4a3be:35437 in memory (size: 37.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1efc78f4a3be:35437 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1efc78f4a3be:35437 in memory (size: 37.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 07:04:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#24)\n",
            "25/08/06 07:04:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 07:04:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#84)\n",
            "25/08/06 07:04:36 INFO CodeGenerator: Code generated in 480.389953 ms\n",
            "25/08/06 07:04:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:04:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:04:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1efc78f4a3be:35437 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:36 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:04:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:04:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:04:36 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:04:36 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:04:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:36 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:04:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:04:36 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 07:04:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1efc78f4a3be:35437 (size: 6.1 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:36 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7991 bytes) \n",
            "25/08/06 07:04:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 07:04:36 INFO FileScanRDD: Reading File path: file:///content/paraquets/customers.parquet/part-00000-c6530b5d-bf16-49ec-b12d-d41e861a942c-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 07:04:36 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 07:04:36 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 07:04:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 3437 bytes result sent to driver\n",
            "25/08/06 07:04:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 838 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:37 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.857 s\n",
            "25/08/06 07:04:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 07:04:37 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.870510 s\n",
            "25/08/06 07:04:37 INFO CodeGenerator: Code generated in 42.686178 ms\n",
            "25/08/06 07:04:37 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 MiB, free 1735.3 MiB)\n",
            "25/08/06 07:04:37 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 1735.3 MiB)\n",
            "25/08/06 07:04:37 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1efc78f4a3be:35437 (size: 2.8 KiB, free: 1767.6 MiB)\n",
            "25/08/06 07:04:37 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:04:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 07:04:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#84)\n",
            "25/08/06 07:04:37 INFO CodeGenerator: Code generated in 212.818285 ms\n",
            "25/08/06 07:04:37 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.2 KiB, free 1735.1 MiB)\n",
            "25/08/06 07:04:37 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1735.1 MiB)\n",
            "25/08/06 07:04:37 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1efc78f4a3be:35437 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:37 INFO SparkContext: Created broadcast 9 from show at task3.java:20\n",
            "25/08/06 07:04:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Registering RDD 19 (show at task3.java:20) as input to shuffle 0\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Got map stage job 7 (show at task3.java:20) with 1 output partitions\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (show at task3.java:20)\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[19] at show at task3.java:20), which has no missing parents\n",
            "25/08/06 07:04:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 48.6 KiB, free 1735.1 MiB)\n",
            "25/08/06 07:04:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 1735.0 MiB)\n",
            "25/08/06 07:04:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 1efc78f4a3be:35437 (size: 21.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[19] at show at task3.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes) \n",
            "25/08/06 07:04:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 07:04:38 INFO CodeGenerator: Code generated in 48.15311 ms\n",
            "25/08/06 07:04:38 INFO CodeGenerator: Code generated in 27.699218 ms\n",
            "25/08/06 07:04:38 INFO CodeGenerator: Code generated in 24.223695 ms\n",
            "25/08/06 07:04:38 INFO CodeGenerator: Code generated in 22.639398 ms\n",
            "25/08/06 07:04:38 INFO FileScanRDD: Reading File path: file:///content/paraquets/payments.parquet/part-00000-29e5902f-b42a-4078-80c0-ce4b0a8700c0-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 07:04:38 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 07:04:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3926 bytes result sent to driver\n",
            "25/08/06 07:04:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 426 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:38 INFO DAGScheduler: ShuffleMapStage 7 (show at task3.java:20) finished in 0.474 s\n",
            "25/08/06 07:04:38 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:04:38 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:04:38 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:04:38 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:04:38 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:04:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:04:38 INFO CodeGenerator: Code generated in 72.58218 ms\n",
            "25/08/06 07:04:38 INFO SparkContext: Starting job: show at task3.java:20\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Got job 8 (show at task3.java:20) with 1 output partitions\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Final stage: ResultStage 9 (show at task3.java:20)\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 1efc78f4a3be:35437 in memory (size: 21.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at show at task3.java:20), which has no missing parents\n",
            "25/08/06 07:04:38 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 45.8 KiB, free 1735.1 MiB)\n",
            "25/08/06 07:04:38 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 1735.1 MiB)\n",
            "25/08/06 07:04:38 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 1efc78f4a3be:35437 (size: 21.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:38 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at show at task3.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:38 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:38 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (1efc78f4a3be, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:04:38 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
            "25/08/06 07:04:38 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1efc78f4a3be:35437 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:38 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:04:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms\n",
            "25/08/06 07:04:38 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 6942 bytes result sent to driver\n",
            "25/08/06 07:04:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 122 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:38 INFO DAGScheduler: ResultStage 9 (show at task3.java:20) finished in 0.155 s\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 07:04:38 INFO DAGScheduler: Job 8 finished: show at task3.java:20, took 0.191808 s\n",
            "25/08/06 07:04:39 INFO CodeGenerator: Code generated in 25.327147 ms\n",
            "+-----------+------------------+\n",
            "|    country|       sum(amount)|\n",
            "+-----------+------------------+\n",
            "|     Sweden|         120457.09|\n",
            "|Philippines|           87468.3|\n",
            "|  Singapore|261671.59999999998|\n",
            "|    Germany|         196470.99|\n",
            "|     France| 965750.5800000001|\n",
            "|    Belgium|          91471.03|\n",
            "|    Finland|         295149.35|\n",
            "|      Italy|325254.55000000005|\n",
            "|     Norway|         104224.79|\n",
            "|      Spain| 994438.5300000003|\n",
            "|    Denmark|          197356.3|\n",
            "|    Ireland|49898.270000000004|\n",
            "|  Hong Kong|          45480.79|\n",
            "|        USA|3040029.5199999996|\n",
            "|   Norway  |         166621.51|\n",
            "|         UK|391503.89999999997|\n",
            "|Switzerland|         108777.92|\n",
            "|     Canada|         205911.86|\n",
            "|      Japan|         167909.95|\n",
            "|New Zealand|         392486.59|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 07:04:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 07:04:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#24)\n",
            "25/08/06 07:04:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 07:04:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#84)\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.2 KiB, free 1734.9 MiB)\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1734.8 MiB)\n",
            "25/08/06 07:04:39 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 1efc78f4a3be:35437 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:39 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:04:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:04:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 13.6 KiB, free 1734.8 MiB)\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1734.8 MiB)\n",
            "25/08/06 07:04:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 1efc78f4a3be:35437 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:39 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:39 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:39 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7991 bytes) \n",
            "25/08/06 07:04:39 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)\n",
            "25/08/06 07:04:39 INFO FileScanRDD: Reading File path: file:///content/paraquets/customers.parquet/part-00000-c6530b5d-bf16-49ec-b12d-d41e861a942c-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 07:04:39 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 07:04:39 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 3394 bytes result sent to driver\n",
            "25/08/06 07:04:39 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 71 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:39 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:39 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.094 s\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 07:04:39 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.128556 s\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 MiB, free 1702.8 MiB)\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 1702.8 MiB)\n",
            "25/08/06 07:04:39 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1efc78f4a3be:35437 (size: 2.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 07:04:39 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 07:04:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 07:04:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#84)\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.2 KiB, free 1702.6 MiB)\n",
            "25/08/06 07:04:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1702.6 MiB)\n",
            "25/08/06 07:04:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 1efc78f4a3be:35437 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:04:39 INFO SparkContext: Created broadcast 15 from parquet at task3.java:21\n",
            "25/08/06 07:04:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Registering RDD 30 (parquet at task3.java:21) as input to shuffle 1\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Got map stage job 10 (parquet at task3.java:21) with 1 output partitions\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (parquet at task3.java:21)\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[30] at parquet at task3.java:21), which has no missing parents\n",
            "25/08/06 07:04:40 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 48.8 KiB, free 1702.5 MiB)\n",
            "25/08/06 07:04:40 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 1702.5 MiB)\n",
            "25/08/06 07:04:40 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 1efc78f4a3be:35437 (size: 21.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 07:04:40 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[30] at parquet at task3.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:40 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:40 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes) \n",
            "25/08/06 07:04:40 INFO Executor: Running task 0.0 in stage 11.0 (TID 10)\n",
            "25/08/06 07:04:40 INFO FileScanRDD: Reading File path: file:///content/paraquets/payments.parquet/part-00000-29e5902f-b42a-4078-80c0-ce4b0a8700c0-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 07:04:40 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 07:04:40 INFO Executor: Finished task 0.0 in stage 11.0 (TID 10). 3926 bytes result sent to driver\n",
            "25/08/06 07:04:40 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 197 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:40 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:40 INFO DAGScheduler: ShuffleMapStage 11 (parquet at task3.java:21) finished in 0.228 s\n",
            "25/08/06 07:04:40 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 07:04:40 INFO DAGScheduler: running: Set()\n",
            "25/08/06 07:04:40 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 07:04:40 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 07:04:40 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 07:04:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:04:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 07:04:40 INFO CodeGenerator: Code generated in 87.929098 ms\n",
            "25/08/06 07:04:40 INFO SparkContext: Starting job: parquet at task3.java:21\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Got job 11 (parquet at task3.java:21) with 1 output partitions\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at task3.java:21)\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[33] at parquet at task3.java:21), which has no missing parents\n",
            "25/08/06 07:04:40 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 245.0 KiB, free 1702.3 MiB)\n",
            "25/08/06 07:04:40 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 1702.2 MiB)\n",
            "25/08/06 07:04:40 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 1efc78f4a3be:35437 (size: 91.3 KiB, free: 1767.3 MiB)\n",
            "25/08/06 07:04:40 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 07:04:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[33] at parquet at task3.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 07:04:40 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 07:04:40 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (1efc78f4a3be, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 07:04:40 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)\n",
            "25/08/06 07:04:40 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 07:04:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 07:04:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 07:04:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 07:04:40 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:04:40 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 07:04:41 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 07:04:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"sum(amount)\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double sum(amount);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 07:04:41 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 07:04:41 INFO FileOutputCommitter: Saved output of task 'attempt_202508060704404788973197648805712_0013_m_000000_11' to file:/content/paraquets/totalRevenueCountryDs.parquet/_temporary/0/task_202508060704404788973197648805712_0013_m_000000\n",
            "25/08/06 07:04:41 INFO SparkHadoopMapRedUtil: attempt_202508060704404788973197648805712_0013_m_000000_11: Committed. Elapsed time: 7 ms.\n",
            "25/08/06 07:04:41 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 7298 bytes result sent to driver\n",
            "25/08/06 07:04:41 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 713 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 07:04:41 INFO DAGScheduler: ResultStage 13 (parquet at task3.java:21) finished in 0.793 s\n",
            "25/08/06 07:04:41 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 07:04:41 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 07:04:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 07:04:41 INFO DAGScheduler: Job 11 finished: parquet at task3.java:21, took 0.834356 s\n",
            "25/08/06 07:04:41 INFO FileFormatWriter: Start to commit write Job e1a533d9-693d-4df2-8c3d-bd81cfadc554.\n",
            "25/08/06 07:04:41 INFO FileFormatWriter: Write Job e1a533d9-693d-4df2-8c3d-bd81cfadc554 committed. Elapsed time: 38 ms.\n",
            "25/08/06 07:04:41 INFO FileFormatWriter: Finished processing stats for write job e1a533d9-693d-4df2-8c3d-bd81cfadc554.\n",
            "25/08/06 07:04:41 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 07:04:41 INFO SparkUI: Stopped Spark web UI at http://1efc78f4a3be:4040\n",
            "25/08/06 07:04:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 07:04:41 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 07:04:41 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 07:04:41 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 07:04:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 07:04:41 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 07:04:41 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 07:04:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-30ac4102-7029-4107-95dd-6922da5bbaad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKnfs0OJIr_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}