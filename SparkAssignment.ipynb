{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhaan-hussain/PySpark/blob/main/SparkAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Scala Env."
      ],
      "metadata": {
        "id": "c6Z58K9N1_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78Oxr-PY1pA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7e66c9-3988-4199-849a-2672994507eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xyJiL9W-2lXu",
        "outputId": "f5174151-b528-45e8-d73c-669519e271fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Hello World in Scala"
      ],
      "metadata": {
        "id": "P86TPUCo6-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "qgcRIiBmsO-P",
        "outputId": "cb078933-4c2f-434f-b352-6aa6ca0876dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.4.1-bin-hadoop3  tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_aQP7Q28De",
        "outputId": "fe8514b4-b9a6-492d-ca01-9ed6d611ec55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/08/06 04:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Spark context Web UI available at http://1efc78f4a3be:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1754456202483).\n",
            "Spark session available as 'spark'.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "         \n",
            "Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 11.0.28)\n",
            "Type in expressions to have them evaluated.\n",
            "Type :help for more information.\n",
            "\u001b[35m\n",
            "scala> \u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "1gIQxLWBtq8J",
        "outputId": "a8c7a0fa-1cb4-47f1-d9bf-3ec96fd76864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.java  offices.csv\t   productlines.csv\t    tmp\n",
            "customers.csv\t orderdetails.csv  products.csv\n",
            "drive\t\t orders.csv\t   sample_data\n",
            "employees.csv\t payments.csv\t   spark-3.4.1-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oppMHVdzufkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Assignment.java\n",
        "import org.apache.spark.sql.*;\n",
        "import java.util.*;\n",
        "\n",
        "public class Assignment\n",
        "{\n",
        "  public static void main(String[] args)\n",
        "  {\n",
        "    SparkSession spark = SparkSession.builder().appName(\"Assignment\").getOrCreate();\n",
        "    DataSet<Row> customersDs = spark.read().option(\"header\", \"true\").csv(\"customers.csv\");\n",
        "    DataSet<Row> ordersDs = spark.read().option(\"header\", \"true\").csv(\"orders.csv\");\n",
        "    DataSet<Row> orderDetailsDs = spark.read().option(\"header\", \"true\").csv(\"orderdetails.csv\");\n",
        "    DataSet<Row> productsDs = spark.read().option(\"header\", \"true\").csv(\"products.csv\");\n",
        "    DataSet<Row> employeesDs = spark.read().option(\"header\", \"true\").csv(\"employees.csv\");\n",
        "    DataSet<Row> officesDs = spark.read().option(\"header\", \"true\").csv(\"offices.csv\");\n",
        "    DataSet<Row> productlineDs = spark.read().option(\"header\", \"true\").csv(\"productlines.csv\");\n",
        "    DataSet<Row> paymentsDs = spark.read().option(\"header\", \"true\").csv(\"payments.csv\");\n",
        "\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIaNIoC56kLN",
        "outputId": "e9e00fa8-d2ba-406e-92d0-32a1f5bf3d85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Assignment.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GuO1JQKwao5",
        "outputId": "c65920fc-98f2-4926-fc08-1c887ee91021"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.java  drive\t    sample_data\t\t     tmp\n",
            "csv_files\t paraquets  spark-3.4.1-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir paraquets"
      ],
      "metadata": {
        "id": "zCuZQUeAxz67"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Assignment.java\n",
        "import org.apache.spark.sql.*;\n",
        "import java.util.*;\n",
        "\n",
        "public class Assignment\n",
        "{\n",
        "  public static void main(String[] args)\n",
        "  {\n",
        "    SparkSession spark = SparkSession.builder().appName(\"Assignment\").master(\"local[*]\").getOrCreate();\n",
        "    Dataset<Row> customersDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/customers.csv\");\n",
        "    Dataset<Row> ordersDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/orders.csv\");\n",
        "    Dataset<Row> orderDetailsDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/orderdetails.csv\");\n",
        "    Dataset<Row> productsDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/products.csv\");\n",
        "    Dataset<Row> employeesDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/employees.csv\");\n",
        "    Dataset<Row> officesDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/offices.csv\");\n",
        "    Dataset<Row> productlineDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/productlines.csv\");\n",
        "    Dataset<Row> paymentsDs = spark.read().option(\"header\", \"true\").csv(\"/content/csv_files/payments.csv\");\n",
        "\n",
        "    customersDs.write().parquet(\"/content/paraquets/customers.parquet\");\n",
        "    ordersDs.write().parquet(\"/content/paraquets/orders.parquet\");\n",
        "    orderDetailsDs.write().parquet(\"/content/paraquets/orderdetails.parquet\");\n",
        "    productsDs.write().parquet(\"/content/paraquets/products.parquet\");\n",
        "    employeesDs.write().parquet(\"/content/paraquets/employees.parquet\");\n",
        "    officesDs.write().parquet(\"/content/paraquets/offices.parquet\");\n",
        "    productlineDs.write().parquet(\"/content/paraquets/productlines.parquet\");\n",
        "    paymentsDs.write().parquet(\"/content/paraquets/payments.parquet\");\n",
        "\n",
        "    spark.stop();\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu5d0K9ex7eY",
        "outputId": "a17d6254-7f15-4269-e831-1610d67ff04c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Assignment.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" Assignment.java"
      ],
      "metadata": {
        "id": "mb6kQ--NyI2U"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(find /content/spark-3.4.1-bin-hadoop3/jars/ -name '*.jar' | tr '\\n' ':')\" \\\n",
        "  --add-exports java.base/sun.nio.ch=ALL-UNNAMED Assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjgemjUbzFBX",
        "outputId": "5f5dcdf6-41d1-4525-ac7f-514702172186"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:21:40 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:21:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:21:41 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:21:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:21:41 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:21:41 INFO SparkContext: Submitted application: Assignment\n",
            "25/08/06 05:21:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:21:41 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:21:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:21:41 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:21:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:21:41 INFO Utils: Successfully started service 'sparkDriver' on port 46467.\n",
            "25/08/06 05:21:41 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:21:41 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:21:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:21:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:21:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:21:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a4d088f6-af42-4d98-ab4f-def20350e30a\n",
            "25/08/06 05:21:42 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:21:42 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:21:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:21:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:21:42 INFO Executor: Starting executor ID driver on host 1efc78f4a3be\n",
            "25/08/06 05:21:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:21:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39867.\n",
            "25/08/06 05:21:42 INFO NettyBlockTransferService: Server created on 1efc78f4a3be:39867\n",
            "25/08/06 05:21:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:21:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:42 INFO BlockManagerMasterEndpoint: Registering block manager 1efc78f4a3be:39867 with 1767.6 MiB RAM, BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1efc78f4a3be, 39867, None)\n",
            "25/08/06 05:21:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:21:43 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:21:45 INFO InMemoryFileIndex: It took 94 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:45 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:51 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 05:21:52 INFO CodeGenerator: Code generated in 469.097519 ms\n",
            "25/08/06 05:21:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:21:53 INFO SparkContext: Created broadcast 0 from csv at Assignment.java:9\n",
            "25/08/06 05:21:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:53 INFO SparkContext: Starting job: csv at Assignment.java:9\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Got job 0 (csv at Assignment.java:9) with 1 output partitions\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Final stage: ResultStage 0 (csv at Assignment.java:9)\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Assignment.java:9), which has no missing parents\n",
            "25/08/06 05:21:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:21:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:21:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Assignment.java:9) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:21:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 05:21:54 INFO FileScanRDD: Reading File path: file:///content/csv_files/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 05:21:54 INFO CodeGenerator: Code generated in 50.382793 ms\n",
            "25/08/06 05:21:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1743 bytes result sent to driver\n",
            "25/08/06 05:21:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 622 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:54 INFO DAGScheduler: ResultStage 0 (csv at Assignment.java:9) finished in 0.978 s\n",
            "25/08/06 05:21:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 05:21:54 INFO DAGScheduler: Job 0 finished: csv at Assignment.java:9, took 1.108503 s\n",
            "25/08/06 05:21:54 INFO CodeGenerator: Code generated in 32.281316 ms\n",
            "25/08/06 05:21:54 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:21:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:21:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:54 INFO SparkContext: Created broadcast 2 from csv at Assignment.java:9\n",
            "25/08/06 05:21:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:54 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:54 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#43, None)) > 0)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 3 from csv at Assignment.java:10\n",
            "25/08/06 05:21:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:55 INFO SparkContext: Starting job: csv at Assignment.java:10\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Got job 1 (csv at Assignment.java:10) with 1 output partitions\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Final stage: ResultStage 1 (csv at Assignment.java:10)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at csv at Assignment.java:10), which has no missing parents\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at csv at Assignment.java:10) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) \n",
            "25/08/06 05:21:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 05:21:55 INFO FileScanRDD: Reading File path: file:///content/csv_files/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 05:21:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1679 bytes result sent to driver\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: ResultStage 1 (csv at Assignment.java:10) finished in 0.090 s\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 1 finished: csv at Assignment.java:10, took 0.101584 s\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 198.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 5 from csv at Assignment.java:10\n",
            "25/08/06 05:21:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#74, None)) > 0)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 198.5 KiB, free 1766.5 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 6 from csv at Assignment.java:11\n",
            "25/08/06 05:21:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:55 INFO SparkContext: Starting job: csv at Assignment.java:11\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Got job 2 (csv at Assignment.java:11) with 1 output partitions\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Final stage: ResultStage 2 (csv at Assignment.java:11)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[23] at csv at Assignment.java:11), which has no missing parents\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.1 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:55 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:55 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[23] at csv at Assignment.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:21:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 05:21:55 INFO FileScanRDD: Reading File path: file:///content/csv_files/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 05:21:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1678 bytes result sent to driver\n",
            "25/08/06 05:21:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 68 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:55 INFO DAGScheduler: ResultStage 2 (csv at Assignment.java:11) finished in 0.111 s\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 05:21:55 INFO DAGScheduler: Job 2 finished: csv at Assignment.java:11, took 0.130177 s\n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 198.5 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 8 from csv at Assignment.java:11\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#101, None)) > 0)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 198.5 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 9 from csv at Assignment.java:12\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO SparkContext: Starting job: csv at Assignment.java:12\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Got job 3 (csv at Assignment.java:12) with 1 output partitions\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Final stage: ResultStage 3 (csv at Assignment.java:12)\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[33] at csv at Assignment.java:12), which has no missing parents\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.1 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[33] at csv at Assignment.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:21:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 05:21:56 INFO FileScanRDD: Reading File path: file:///content/csv_files/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 05:21:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1698 bytes result sent to driver\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 65 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:56 INFO DAGScheduler: ResultStage 3 (csv at Assignment.java:12) finished in 0.096 s\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 3 finished: csv at Assignment.java:12, took 0.110217 s\n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 198.5 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 11 from csv at Assignment.java:12\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:56 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#136, None)) > 0)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.5 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 12 from csv at Assignment.java:13\n",
            "25/08/06 05:21:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:56 INFO SparkContext: Starting job: csv at Assignment.java:13\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Got job 4 (csv at Assignment.java:13) with 1 output partitions\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Final stage: ResultStage 4 (csv at Assignment.java:13)\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[43] at csv at Assignment.java:13), which has no missing parents\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.1 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:56 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:56 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[43] at csv at Assignment.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:21:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 05:21:56 INFO FileScanRDD: Reading File path: file:///content/csv_files/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 05:21:56 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1646 bytes result sent to driver\n",
            "25/08/06 05:21:56 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 33 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:56 INFO DAGScheduler: ResultStage 4 (csv at Assignment.java:13) finished in 0.051 s\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 05:21:56 INFO DAGScheduler: Job 4 finished: csv at Assignment.java:13, took 0.064690 s\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.5 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 14 from csv at Assignment.java:13\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#169, None)) > 0)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 198.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 15 from csv at Assignment.java:14\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Starting job: csv at Assignment.java:14\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Got job 5 (csv at Assignment.java:14) with 1 output partitions\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Final stage: ResultStage 5 (csv at Assignment.java:14)\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[53] at csv at Assignment.java:14), which has no missing parents\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 12.1 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[53] at csv at Assignment.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 05:21:57 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 05:21:57 INFO FileScanRDD: Reading File path: file:///content/csv_files/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 05:21:57 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1648 bytes result sent to driver\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 29 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:57 INFO DAGScheduler: ResultStage 5 (csv at Assignment.java:14) finished in 0.051 s\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 5 finished: csv at Assignment.java:14, took 0.060428 s\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 198.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 17 from csv at Assignment.java:14\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#204, None)) > 0)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 198.5 KiB, free 1766.5 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 18 from csv at Assignment.java:15\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO SparkContext: Starting job: csv at Assignment.java:15\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Got job 6 (csv at Assignment.java:15) with 1 output partitions\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Final stage: ResultStage 6 (csv at Assignment.java:15)\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[63] at csv at Assignment.java:15), which has no missing parents\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 12.1 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[63] at csv at Assignment.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:21:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 05:21:57 INFO FileScanRDD: Reading File path: file:///content/csv_files/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 05:21:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1617 bytes result sent to driver\n",
            "25/08/06 05:21:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 29 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:57 INFO DAGScheduler: ResultStage 6 (csv at Assignment.java:15) finished in 0.045 s\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 05:21:57 INFO DAGScheduler: Job 6 finished: csv at Assignment.java:15, took 0.050605 s\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 198.5 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 20 from csv at Assignment.java:15\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:57 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#229, None)) > 0)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 198.5 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:57 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.0 MiB)\n",
            "25/08/06 05:21:57 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:57 INFO SparkContext: Created broadcast 21 from csv at Assignment.java:16\n",
            "25/08/06 05:21:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:58 INFO SparkContext: Starting job: csv at Assignment.java:16\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Got job 7 (csv at Assignment.java:16) with 1 output partitions\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Final stage: ResultStage 7 (csv at Assignment.java:16)\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[73] at csv at Assignment.java:16), which has no missing parents\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 12.1 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 1efc78f4a3be:39867 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[73] at csv at Assignment.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:21:58 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 05:21:58 INFO FileScanRDD: Reading File path: file:///content/csv_files/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 05:21:58 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1614 bytes result sent to driver\n",
            "25/08/06 05:21:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 34 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:21:58 INFO DAGScheduler: ResultStage 7 (csv at Assignment.java:16) finished in 0.053 s\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Job 7 finished: csv at Assignment.java:16, took 0.061478 s\n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 198.5 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 23 from csv at Assignment.java:16\n",
            "25/08/06 05:21:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:21:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:21:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 198.5 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.5 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 24 from parquet at Assignment.java:18\n",
            "25/08/06 05:21:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:21:58 INFO SparkContext: Starting job: parquet at Assignment.java:18\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Got job 8 (parquet at Assignment.java:18) with 1 output partitions\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at Assignment.java:18)\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[82] at parquet at Assignment.java:18), which has no missing parents\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 213.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 05:21:58 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 1765.2 MiB)\n",
            "25/08/06 05:21:58 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 1efc78f4a3be:39867 (size: 76.9 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:21:58 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:21:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[82] at parquet at Assignment.java:18) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:21:58 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:21:58 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:21:58 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:21:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:21:58 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:21:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:21:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary salesRepEmployeeNumber (STRING);\n",
            "  optional binary creditLimit (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:21:58 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 1efc78f4a3be:39867 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:59 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:21:59 INFO FileScanRDD: Reading File path: file:///content/csv_files/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 05:21:59 INFO CodeGenerator: Code generated in 43.106402 ms\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: Saved output of task 'attempt_20250806052158197197136055836895_0008_m_000000_8' to file:/content/paraquets/customers.parquet/_temporary/0/task_20250806052158197197136055836895_0008_m_000000\n",
            "25/08/06 05:22:00 INFO SparkHadoopMapRedUtil: attempt_20250806052158197197136055836895_0008_m_000000_8: Committed. Elapsed time: 8 ms.\n",
            "25/08/06 05:22:00 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2545 bytes result sent to driver\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 1588 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:00 INFO DAGScheduler: ResultStage 8 (parquet at Assignment.java:18) finished in 1.670 s\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 8 finished: parquet at Assignment.java:18, took 1.675159 s\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Start to commit write Job 076067cf-9666-4c84-bfcd-93850b53be27.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Write Job 076067cf-9666-4c84-bfcd-93850b53be27 committed. Elapsed time: 41 ms.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Finished processing stats for write job 076067cf-9666-4c84-bfcd-93850b53be27.\n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 26 from parquet at Assignment.java:19\n",
            "25/08/06 05:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:00 INFO SparkContext: Starting job: parquet at Assignment.java:19\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Got job 9 (parquet at Assignment.java:19) with 1 output partitions\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at Assignment.java:19)\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[85] at parquet at Assignment.java:19), which has no missing parents\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 211.5 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 1efc78f4a3be:39867 (size: 76.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[85] at parquet at Assignment.java:19) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7916 bytes) \n",
            "25/08/06 05:22:00 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"requiredDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"shippedDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"status\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"comments\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary orderNumber (STRING);\n",
            "  optional binary orderDate (STRING);\n",
            "  optional binary requiredDate (STRING);\n",
            "  optional binary shippedDate (STRING);\n",
            "  optional binary status (STRING);\n",
            "  optional binary comments (STRING);\n",
            "  optional binary customerNumber (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:00 INFO FileScanRDD: Reading File path: file:///content/csv_files/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 05:22:00 INFO CodeGenerator: Code generated in 51.434523 ms\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522006319789049353731045_0009_m_000000_9' to file:/content/paraquets/orders.parquet/_temporary/0/task_202508060522006319789049353731045_0009_m_000000\n",
            "25/08/06 05:22:00 INFO SparkHadoopMapRedUtil: attempt_202508060522006319789049353731045_0009_m_000000_9: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 05:22:00 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 242 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:00 INFO DAGScheduler: ResultStage 9 (parquet at Assignment.java:19) finished in 0.290 s\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Job 9 finished: parquet at Assignment.java:19, took 0.298467 s\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Start to commit write Job e5a6e2f9-8574-4515-8112-e5bab1f766ad.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Write Job e5a6e2f9-8574-4515-8112-e5bab1f766ad committed. Elapsed time: 21 ms.\n",
            "25/08/06 05:22:00 INFO FileFormatWriter: Finished processing stats for write job e5a6e2f9-8574-4515-8112-e5bab1f766ad.\n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 28 from parquet at Assignment.java:20\n",
            "25/08/06 05:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:00 INFO SparkContext: Starting job: parquet at Assignment.java:20\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Got job 10 (parquet at Assignment.java:20) with 1 output partitions\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at Assignment.java:20)\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[88] at parquet at Assignment.java:20), which has no missing parents\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 211.1 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:22:00 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:22:00 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 1efc78f4a3be:39867 (size: 76.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:22:00 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[88] at parquet at Assignment.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:22:00 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary orderNumber (STRING);\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary quantityOrdered (STRING);\n",
            "  optional binary priceEach (STRING);\n",
            "  optional binary orderLineNumber (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:00 INFO FileScanRDD: Reading File path: file:///content/csv_files/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 05:22:00 INFO CodeGenerator: Code generated in 35.603158 ms\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522001461962991534138916_0010_m_000000_10' to file:/content/paraquets/orderdetails.parquet/_temporary/0/task_202508060522001461962991534138916_0010_m_000000\n",
            "25/08/06 05:22:01 INFO SparkHadoopMapRedUtil: attempt_202508060522001461962991534138916_0010_m_000000_10: Committed. Elapsed time: 5 ms.\n",
            "25/08/06 05:22:01 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:01 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 240 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:01 INFO DAGScheduler: ResultStage 10 (parquet at Assignment.java:20) finished in 0.283 s\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 10 finished: parquet at Assignment.java:20, took 0.288455 s\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Start to commit write Job 97551e3c-c873-45d2-9871-8821d8a07270.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Write Job 97551e3c-c873-45d2-9871-8821d8a07270 committed. Elapsed time: 27 ms.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Finished processing stats for write job 97551e3c-c873-45d2-9871-8821d8a07270.\n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 198.5 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.8 MiB)\n",
            "25/08/06 05:22:01 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:01 INFO SparkContext: Created broadcast 30 from parquet at Assignment.java:21\n",
            "25/08/06 05:22:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:01 INFO SparkContext: Starting job: parquet at Assignment.java:21\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Got job 11 (parquet at Assignment.java:21) with 1 output partitions\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at Assignment.java:21)\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[91] at parquet at Assignment.java:21), which has no missing parents\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 212.0 KiB, free 1765.6 MiB)\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 1765.6 MiB)\n",
            "25/08/06 05:22:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 1efc78f4a3be:39867 (size: 76.6 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:01 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[91] at parquet at Assignment.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:01 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:22:01 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:01 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:01 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productScale\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productVendor\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityInStock\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"buyPrice\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"MSRP\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary productScale (STRING);\n",
            "  optional binary productVendor (STRING);\n",
            "  optional binary productDescription (STRING);\n",
            "  optional binary quantityInStock (STRING);\n",
            "  optional binary buyPrice (STRING);\n",
            "  optional binary MSRP (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:01 INFO FileScanRDD: Reading File path: file:///content/csv_files/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 05:22:01 INFO CodeGenerator: Code generated in 35.486986 ms\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522017822383054524772237_0011_m_000000_11' to file:/content/paraquets/products.parquet/_temporary/0/task_202508060522017822383054524772237_0011_m_000000\n",
            "25/08/06 05:22:01 INFO SparkHadoopMapRedUtil: attempt_202508060522017822383054524772237_0011_m_000000_11: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:01 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:01 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 509 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:01 INFO DAGScheduler: ResultStage 11 (parquet at Assignment.java:21) finished in 0.579 s\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 05:22:01 INFO DAGScheduler: Job 11 finished: parquet at Assignment.java:21, took 0.588556 s\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Start to commit write Job 0cb62299-9a18-41c3-b069-f5428a0bd300.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Write Job 0cb62299-9a18-41c3-b069-f5428a0bd300 committed. Elapsed time: 37 ms.\n",
            "25/08/06 05:22:01 INFO FileFormatWriter: Finished processing stats for write job 0cb62299-9a18-41c3-b069-f5428a0bd300.\n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 198.5 KiB, free 1765.4 MiB)\n",
            "25/08/06 05:22:01 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 32 from parquet at Assignment.java:22\n",
            "25/08/06 05:22:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:02 INFO SparkContext: Starting job: parquet at Assignment.java:22\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Got job 12 (parquet at Assignment.java:22) with 1 output partitions\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Assignment.java:22)\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[94] at parquet at Assignment.java:22), which has no missing parents\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 211.8 KiB, free 1765.1 MiB)\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 1765.1 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 1efc78f4a3be:39867 (size: 76.5 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[94] at parquet at Assignment.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7919 bytes) \n",
            "25/08/06 05:22:02 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"firstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"extension\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"email\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"reportsTo\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"jobTitle\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary employeeNumber (STRING);\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary firstName (STRING);\n",
            "  optional binary extension (STRING);\n",
            "  optional binary email (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary reportsTo (STRING);\n",
            "  optional binary jobTitle (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:02 INFO FileScanRDD: Reading File path: file:///content/csv_files/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 05:22:02 INFO CodeGenerator: Code generated in 70.165751 ms\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522028991684213158794495_0012_m_000000_12' to file:/content/paraquets/employees.parquet/_temporary/0/task_202508060522028991684213158794495_0012_m_000000\n",
            "25/08/06 05:22:02 INFO SparkHadoopMapRedUtil: attempt_202508060522028991684213158794495_0012_m_000000_12: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:02 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2502 bytes result sent to driver\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 220 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:02 INFO DAGScheduler: ResultStage 12 (parquet at Assignment.java:22) finished in 0.279 s\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 12 finished: parquet at Assignment.java:22, took 0.286910 s\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Start to commit write Job 737d908b-2acd-456d-86a6-15850f57c2aa.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Write Job 737d908b-2acd-456d-86a6-15850f57c2aa committed. Elapsed time: 19 ms.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Finished processing stats for write job 737d908b-2acd-456d-86a6-15850f57c2aa.\n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 198.5 KiB, free 1764.9 MiB)\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.8 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 34 from parquet at Assignment.java:23\n",
            "25/08/06 05:22:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:02 INFO SparkContext: Starting job: parquet at Assignment.java:23\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Got job 13 (parquet at Assignment.java:23) with 1 output partitions\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Assignment.java:23)\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[97] at parquet at Assignment.java:23), which has no missing parents\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 212.0 KiB, free 1764.6 MiB)\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1764.5 MiB)\n",
            "25/08/06 05:22:02 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 1efc78f4a3be:39867 (size: 76.4 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:02 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[97] at parquet at Assignment.java:23) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7917 bytes) \n",
            "25/08/06 05:22:02 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:02 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"territory\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary territory (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:02 INFO FileScanRDD: Reading File path: file:///content/csv_files/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522029066046437883789402_0013_m_000000_13' to file:/content/paraquets/offices.parquet/_temporary/0/task_202508060522029066046437883789402_0013_m_000000\n",
            "25/08/06 05:22:02 INFO SparkHadoopMapRedUtil: attempt_202508060522029066046437883789402_0013_m_000000_13: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:02 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:02 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 215 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:02 INFO DAGScheduler: ResultStage 13 (parquet at Assignment.java:23) finished in 0.284 s\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 05:22:02 INFO DAGScheduler: Job 13 finished: parquet at Assignment.java:23, took 0.290576 s\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Start to commit write Job ef241786-2017-4db7-a5c9-780ae32cb6c3.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Write Job ef241786-2017-4db7-a5c9-780ae32cb6c3 committed. Elapsed time: 29 ms.\n",
            "25/08/06 05:22:02 INFO FileFormatWriter: Finished processing stats for write job ef241786-2017-4db7-a5c9-780ae32cb6c3.\n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:02 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 198.5 KiB, free 1764.4 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.3 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1766.9 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 36 from parquet at Assignment.java:24\n",
            "25/08/06 05:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:03 INFO SparkContext: Starting job: parquet at Assignment.java:24\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Got job 14 (parquet at Assignment.java:24) with 1 output partitions\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Assignment.java:24)\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[100] at parquet at Assignment.java:24), which has no missing parents\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 210.9 KiB, free 1764.1 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1764.0 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 1efc78f4a3be:39867 (size: 76.3 KiB, free: 1766.8 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[100] at parquet at Assignment.java:24) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:03 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7922 bytes) \n",
            "25/08/06 05:22:03 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 1efc78f4a3be:39867 in memory (size: 76.6 KiB, free: 1766.9 MiB)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"htmlDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"image\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "  optional binary htmlDescription (STRING);\n",
            "  optional binary image (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 1efc78f4a3be:39867 in memory (size: 76.4 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 1efc78f4a3be:39867 in memory (size: 76.5 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:03 INFO FileScanRDD: Reading File path: file:///content/csv_files/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:22:03 INFO CodeGenerator: Code generated in 31.203658 ms\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522033384327301066927734_0014_m_000000_14' to file:/content/paraquets/productlines.parquet/_temporary/0/task_202508060522033384327301066927734_0014_m_000000\n",
            "25/08/06 05:22:03 INFO SparkHadoopMapRedUtil: attempt_202508060522033384327301066927734_0014_m_000000_14: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:03 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2502 bytes result sent to driver\n",
            "25/08/06 05:22:03 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 422 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:03 INFO DAGScheduler: ResultStage 14 (parquet at Assignment.java:24) finished in 0.524 s\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Job 14 finished: parquet at Assignment.java:24, took 0.529915 s\n",
            "25/08/06 05:22:03 INFO FileFormatWriter: Start to commit write Job 5feaae44-463c-4fd4-a31e-89897092a4fc.\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:03 INFO FileFormatWriter: Write Job 5feaae44-463c-4fd4-a31e-89897092a4fc committed. Elapsed time: 25 ms.\n",
            "25/08/06 05:22:03 INFO FileFormatWriter: Finished processing stats for write job 5feaae44-463c-4fd4-a31e-89897092a4fc.\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:22:03 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:22:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:22:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 1efc78f4a3be:39867 in memory (size: 76.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 1efc78f4a3be:39867 in memory (size: 76.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 1efc78f4a3be:39867 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 1efc78f4a3be:39867 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 1efc78f4a3be:39867 in memory (size: 76.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 38 from parquet at Assignment.java:25\n",
            "25/08/06 05:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:22:03 INFO SparkContext: Starting job: parquet at Assignment.java:25\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Got job 15 (parquet at Assignment.java:25) with 1 output partitions\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at Assignment.java:25)\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[103] at parquet at Assignment.java:25), which has no missing parents\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 210.9 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:22:03 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:22:03 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 1efc78f4a3be:39867 (size: 76.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:22:03 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[103] at parquet at Assignment.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:22:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:22:03 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (1efc78f4a3be, executor driver, partition 0, PROCESS_LOCAL, 7918 bytes) \n",
            "25/08/06 05:22:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:22:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:22:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"amount\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional binary paymentDate (STRING);\n",
            "  optional binary amount (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:22:03 INFO FileScanRDD: Reading File path: file:///content/csv_files/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 05:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_202508060522032169946207929521064_0015_m_000000_15' to file:/content/paraquets/payments.parquet/_temporary/0/task_202508060522032169946207929521064_0015_m_000000\n",
            "25/08/06 05:22:04 INFO SparkHadoopMapRedUtil: attempt_202508060522032169946207929521064_0015_m_000000_15: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:22:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2459 bytes result sent to driver\n",
            "25/08/06 05:22:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 186 ms on 1efc78f4a3be (executor driver) (1/1)\n",
            "25/08/06 05:22:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:22:04 INFO DAGScheduler: ResultStage 15 (parquet at Assignment.java:25) finished in 0.271 s\n",
            "25/08/06 05:22:04 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 05:22:04 INFO DAGScheduler: Job 15 finished: parquet at Assignment.java:25, took 0.274485 s\n",
            "25/08/06 05:22:04 INFO FileFormatWriter: Start to commit write Job 5d634d85-610f-488f-913a-30e31077d583.\n",
            "25/08/06 05:22:04 INFO FileFormatWriter: Write Job 5d634d85-610f-488f-913a-30e31077d583 committed. Elapsed time: 34 ms.\n",
            "25/08/06 05:22:04 INFO FileFormatWriter: Finished processing stats for write job 5d634d85-610f-488f-913a-30e31077d583.\n",
            "25/08/06 05:22:04 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:22:04 INFO SparkUI: Stopped Spark web UI at http://1efc78f4a3be:4040\n",
            "25/08/06 05:22:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:22:04 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:22:04 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:22:04 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:22:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:22:04 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:22:04 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:22:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-1235662a-4791-42bb-ac45-58f82b5ad1cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW2hGhe1zmLS",
        "outputId": "1404007b-3ab9-4490-bfd1-ecfd6f017657"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.class  csv_files  paraquets\t  spark-3.4.1-bin-hadoop3\n",
            "Assignment.java   drive      sample_data  tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glKC8MUk0Bwx",
        "outputId": "6f8e11a8-da18-422c-8cdd-2c98ddaf475b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-u1EmEO0KFj",
        "outputId": "cc406eb2-d946-4745-a819-ed3f5ce74150"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment.class  \u001b[0m\u001b[01;34mcsv_files\u001b[0m/  \u001b[01;34mparaquets\u001b[0m/    \u001b[01;34mspark-3.4.1-bin-hadoop3\u001b[0m/\n",
            "Assignment.java   \u001b[01;34mdrive\u001b[0m/      \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtmp\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5I99iDy1J8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}